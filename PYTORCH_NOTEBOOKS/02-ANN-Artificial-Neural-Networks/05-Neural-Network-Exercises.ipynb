{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Pierian-Data-Logo.PNG\">\n",
    "<br>\n",
    "<strong><center>Copyright 2019. Created by Jose Marcial Portilla.</center></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network Exercises\n",
    "For these exercises we'll perform a binary classification on the Census Income dataset available from the <a href = 'http://archive.ics.uci.edu/ml/datasets/Adult'>UC Irvine Machine Learning Repository</a><br>\n",
    "The goal is to determine if an individual earns more than $50K based on a set of continuous and categorical variables.\n",
    "\n",
    "<div class=\"alert alert-danger\" style=\"margin: 10px\"><strong>IMPORTANT NOTE!</strong> Make sure you don't run the cells directly above the example output shown, <br>otherwise you will end up writing over the example output!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Income Dataset\n",
    "For this exercises we're using the Census Income dataset available from the <a href='http://archive.ics.uci.edu/ml/datasets/Adult'>UC Irvine Machine Learning Repository</a>.\n",
    "\n",
    "The full dataset has 48,842 entries. For this exercise we have reduced the number of records, fields and field entries, and have removed entries with null values. The file <strong>income.csv</strong> has\t30,000 entries\n",
    "\n",
    "Each entry contains the following information about an individual:\n",
    "* <strong>age</strong>: the age of an individual as an integer from 18 to 90 (continuous)\n",
    "* <strong>sex</strong>: Male or Female (categorical)\n",
    "* <strong>education</strong>: represents the highest level of education achieved by an individual (categorical)\n",
    "* <strong>education_num</strong>: represents education as an integer from 3 to 16 (categorical)\n",
    "<div><table style=\"display: inline-block\">\n",
    "<tr><td>3</td><td>5th-6th</td><td>8</td><td>12th</td><td>13</td><td>Bachelors</td></tr>\n",
    "<tr><td>4</td><td>7th-8th</td><td>9</td><td>HS-grad</td><td>14</td><td>Masters</td></tr>\n",
    "<tr><td>5</td><td>9th</td><td>10</td><td>Some-college</td><td>15</td><td>Prof-school</td></tr>\n",
    "<tr><td>6</td><td>10th</td><td>11</td><td>Assoc-voc</td><td>16</td><td>Doctorate</td></tr>\n",
    "<tr><td>7</td><td>11th</td><td>12</td><td>Assoc-acdm</td></tr>\n",
    "</table></div>\n",
    "* <strong>marital-status</strong>: marital status of an individual (categorical)\n",
    "<div><table style=\"display: inline-block\">\n",
    "<tr><td>Married</td><td>Divorced</td><td>Married-spouse-absent</td></tr>\n",
    "<tr><td>Separated</td><td>Widowed</td><td>Never-married</td></tr>\n",
    "</table></div>\n",
    "* <strong>workclass</strong>: a general term to represent the employment status of an individual (categorical)\n",
    "<div><table style=\"display: inline-block\">\n",
    "<tr><td>Local-gov</td><td>Private</td></tr>\n",
    "<tr><td>State-gov</td><td>Self-emp</td></tr>\n",
    "<tr><td>Federal-gov</td></tr>\n",
    "</table></div>\n",
    "* <strong>occupation</strong>: the general type of occupation of an individual (categorical)\n",
    "<div><table style=\"display: inline-block\">\n",
    "<tr><td>Adm-clerical</td><td>Handlers-cleaners</td><td>Protective-serv</td></tr>\n",
    "<tr><td>Craft-repair</td><td>Machine-op-inspct</td><td>Sales</td></tr>\n",
    "<tr><td>Exec-managerial</td><td>Other-service</td><td>Tech-support</td></tr>\n",
    "<tr><td>Farming-fishing</td><td>Prof-specialty</td><td>Transport-moving</td></tr>\n",
    "</table></div>\n",
    "* <strong>hours-per-week</strong>: the hours an individual has reported to work per week as an integer from 20 to 90 (continuous)\n",
    "* <strong>income</strong>: whether or not an individual makes more than \\\\$50,000 annually (label)\n",
    "* <strong>label</strong>: income represented as an integer (0: <=\\\\$50K, 1: >\\\\$50K) (optional label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform standard imports\n",
    "Run the cell below to load the libraries needed for this exercise and the Census Income dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('../Data/income.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>workclass</th>\n",
       "      <th>occupation</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Private</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>Male</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>50</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Self-emp</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>20</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>Female</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>57</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>Female</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Private</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex    education  education-num marital-status    workclass  \\\n",
       "0   27    Male      HS-grad              9  Never-married      Private   \n",
       "1   47    Male      Masters             14        Married    Local-gov   \n",
       "2   59    Male      HS-grad              9       Divorced     Self-emp   \n",
       "3   38  Female  Prof-school             15  Never-married  Federal-gov   \n",
       "4   64  Female         11th              7        Widowed      Private   \n",
       "\n",
       "        occupation  hours-per-week income  label  \n",
       "0     Craft-repair              40  <=50K      0  \n",
       "1  Exec-managerial              50   >50K      1  \n",
       "2   Prof-specialty              20  <=50K      0  \n",
       "3   Prof-specialty              57   >50K      1  \n",
       "4  Farming-fishing              40  <=50K      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21700\n",
       "1     8300\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Separate continuous, categorical and label column names\n",
    "You should find that there are 5 categorical columns, 2 continuous columns and 1 label.<br>\n",
    "In the case of <em>education</em> and <em>education-num</em> it doesn't matter which column you use. For the label column, be sure to use <em>label</em> and not <em>income</em>.<br>\n",
    "Assign the variable names \"cat_cols\", \"cont_cols\" and \"y_col\" to the lists of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols  has 5 columns\n",
      "cont_cols has 2 columns\n",
      "y_col     has 1 column\n"
     ]
    }
   ],
   "source": [
    "# CODE HERE\n",
    "    \n",
    "cat_cols = [\"sex\",\"education\",\"education-num\",\"workclass\",\"occupation\"]\n",
    "cont_cols = [\"hours-per-week\",\"age\"]\n",
    "y_col = [\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "# RUN THIS CODE TO COMPARE RESULTS:\n",
    "print(f'cat_cols  has {len(cat_cols)} columns')\n",
    "print(f'cont_cols has {len(cont_cols)} columns')\n",
    "print(f'y_col     has {len(y_col)} column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols  has 5 columns\n",
      "cont_cols has 2 columns\n",
      "y_col     has 1 column\n"
     ]
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert categorical columns to category dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "\n",
    "\n",
    "for cat in cat_cols:\n",
    "    df[cat] = df[cat].astype('category')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Shuffle the dataset\n",
    "The <strong>income.csv</strong> dataset is already shuffled. However, if you would like to try different configurations after completing the exercises, this is where you would want to shuffle the entire set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>workclass</th>\n",
       "      <th>occupation</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>Female</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Private</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>Female</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Married</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>39</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>Male</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Private</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married</td>\n",
       "      <td>Private</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>Female</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Private</td>\n",
       "      <td>Sales</td>\n",
       "      <td>25</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     education education-num marital-status  workclass  \\\n",
       "0   23  Female       HS-grad             9  Never-married    Private   \n",
       "1   37  Female   Prof-school            15        Married  State-gov   \n",
       "2   34    Male  Some-college            10       Divorced    Private   \n",
       "3   31    Male       HS-grad             9        Married    Private   \n",
       "4   20  Female  Some-college            10  Never-married    Private   \n",
       "\n",
       "       occupation  hours-per-week income  label  \n",
       "0   Other-service              50  <=50K      0  \n",
       "1  Prof-specialty              39   >50K      1  \n",
       "2    Adm-clerical              40  <=50K      0  \n",
       "3    Craft-repair              40   >50K      1  \n",
       "4           Sales              25  <=50K      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS CELL IS OPTIONAL\n",
    "df = shuffle(df, random_state=101)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set the embedding sizes\n",
    "Create a variable \"cat_szs\" to hold the number of categories in each variable.<br>\n",
    "Then create a variable \"emb_szs\" to hold the list of (category size, embedding size) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 14, 14, 5, 12]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE HERE\n",
    "cat_szs = [len(df[col].cat.categories) for col in cat_cols]\n",
    "emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\n",
    "cat_szs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (14, 7), (6, 3), (5, 3), (12, 6)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create an array of categorical values\n",
    "Create a NumPy array called \"cats\" that contains a stack of each categorical column <tt>.cat.codes.values</tt><br>\n",
    "Note: your output may contain different values. Ours came after performing the shuffle step shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 10,  6,  2,  6],\n",
       "       [ 0, 12, 12,  4,  7],\n",
       "       [ 1, 13,  7,  2,  0],\n",
       "       [ 1, 10,  6,  2,  1],\n",
       "       [ 0, 13,  7,  2,  9]], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE HERE\n",
    "\n",
    "sex = df['sex'].cat.codes.values\n",
    "education = df['education'].cat.codes.values\n",
    "education_num = df['education-num'].cat.codes.values\n",
    "workclass = df['workclass'].cat.codes.values\n",
    "occupation = df['occupation'].cat.codes.values\n",
    "\n",
    "cats = np.stack([sex, education, education_num, workclass, occupation], 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RUN THIS CODE TO COMPARE RESULTS\n",
    "cats[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 10,  3,  2,  1],\n",
       "       [ 1, 11,  1,  1,  2],\n",
       "       [ 1, 10,  0,  3,  7],\n",
       "       [ 0, 12,  3,  0,  7],\n",
       "       [ 0,  1,  5,  2,  3]], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convert \"cats\" to a tensor\n",
    "Convert the \"cats\" NumPy array to a tensor of dtype <tt>int64</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "\n",
    "cats = torch.tensor(cats, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create an array of continuous values\n",
    "Create a NumPy array called \"conts\" that contains a stack of each continuous column.<br>\n",
    "Note: your output may contain different values. Ours came after performing the shuffle step shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50, 23],\n",
       "       [39, 37],\n",
       "       [40, 34],\n",
       "       [40, 31],\n",
       "       [25, 20]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE HERE\n",
    "\n",
    "conts = np.stack([df[col].values for col in cont_cols], 1)\n",
    "\n",
    "# RUN THIS CODE TO COMPARE RESULTS\n",
    "conts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 40],\n",
       "       [47, 50],\n",
       "       [59, 20],\n",
       "       [38, 57],\n",
       "       [64, 40]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Convert \"conts\" to a tensor\n",
    "Convert the \"conts\" NumPy array to a tensor of dtype <tt>float32</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE HERE\n",
    "conts = torch.tensor(conts, dtype=torch.float)\n",
    "\n",
    "# RUN THIS CODE TO COMPARE RESULTS\n",
    "conts.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Create a label tensor\n",
    "Create a tensor called \"y\" from the values in the label column. Be sure to flatten the tensor so that it can be passed into the CE Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "y = torch.tensor(df[y_col].values).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Create train and test sets from <tt>cats</tt>, <tt>conts</tt>, and <tt>y</tt>\n",
    "We use the entire batch of 30,000 records, but a smaller batch size will save time during training.<br>\n",
    "We used a test size of 5,000 records, but you can choose another fixed value or a percentage of the batch size.<br>\n",
    "Make sure that your test records remain separate from your training records, without overlap.<br>\n",
    "To make coding slices easier, we recommend assigning batch and test sizes to simple variables like \"b\" and \"t\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 5000\n"
     ]
    }
   ],
   "source": [
    "# CODE HERE\n",
    "b = 30000 # suggested batch size\n",
    "t = 5000  # suggested test size\n",
    "cat_train = cats[t:]\n",
    "cat_test = cats[:t]\n",
    "con_train = conts[t:]\n",
    "con_test = conts[:t]\n",
    "y_train = y[t:]\n",
    "y_test = y[:t]\n",
    "\n",
    "print(len(cats_train), len(cats_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model class\n",
    "Run the cell below to define the TabularModel model class we've used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        # Call the parent __init__\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up the embedding, dropout, and batch normalization layer attributes\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        # Assign a variable to hold a list of layers\n",
    "        layerlist = []\n",
    "        \n",
    "        # Assign a variable to store the number of embedding and continuous layers\n",
    "        n_emb = sum((nf for ni,nf in emb_szs))\n",
    "        n_in = n_emb + n_cont\n",
    "        \n",
    "        # Iterate through the passed-in \"layers\" parameter (ie, [200,100]) to build a list of layers\n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i)) \n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "        \n",
    "        # Convert the list of layers into an attribute\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        # Extract embedding values from the incoming categorical data\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        # Perform an initial dropout on the embeddings\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        # Normalize the incoming continuous data\n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        \n",
    "        # Set up model layers\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Set the random seed\n",
    "To obtain results that can be recreated, set a torch manual_seed (we used 33)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116996470>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE HERE\n",
    "torch.manual_seed(33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e5e64e5e30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Create a TabularModel instance\n",
    "Create an instance called \"model\" with one hidden layer containing 50 neurons and a dropout layer p-value of 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(2, 1)\n",
       "    (1): Embedding(14, 7)\n",
       "    (2): Embedding(14, 7)\n",
       "    (3): Embedding(5, 3)\n",
       "    (4): Embedding(12, 6)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4)\n",
       "  (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=26, out_features=50, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4)\n",
       "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE HERE\n",
    "model = TabularModel(emb_szs, conts.shape[1], 2, [50], p=0.4)\n",
    "\n",
    "# RUN THIS CODE TO COMPARE RESULTS\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(2, 1)\n",
       "    (1): Embedding(14, 7)\n",
       "    (2): Embedding(6, 3)\n",
       "    (3): Embedding(5, 3)\n",
       "    (4): Embedding(12, 6)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4)\n",
       "  (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=22, out_features=50, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4)\n",
       "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Define the loss and optimization functions\n",
    "Create a loss function called \"criterion\" using CrossEntropyLoss<br>\n",
    "Create an optimization function called \"optimizer\" using Adam, with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 0, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Run the cell below to train the model through 300 epochs. Remember, results may vary!<br>\n",
    "After completing the exercises, feel free to come back to this section and experiment with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.5308],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.2789, 0.5872],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0431]], grad_fn=<ReluBackward0>)\n",
      "epoch:   1  loss: 0.76061320\n",
      "tensor([[0.0264, 0.0000],\n",
      "        [0.6377, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [0.4299, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3320, 0.5568],\n",
      "        [0.7797, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1635, 0.3758],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.1859, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.3232],\n",
      "        [0.8103, 0.0000],\n",
      "        [0.0000, 0.1822],\n",
      "        ...,\n",
      "        [0.2310, 0.2731],\n",
      "        [0.1788, 0.0000],\n",
      "        [0.7440, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6400, 0.1005],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.1553, 0.0000],\n",
      "        ...,\n",
      "        [0.1118, 0.3721],\n",
      "        [0.4441, 0.0000],\n",
      "        [0.0000, 0.1833]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.2489],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4461],\n",
      "        [0.0594, 0.0000],\n",
      "        [0.0780, 0.1028]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.3314, 0.0349],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2106],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0160, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.1194],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.3545],\n",
      "        ...,\n",
      "        [0.0000, 0.5739],\n",
      "        [0.4729, 0.3429],\n",
      "        [0.0526, 0.9009]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1736, 0.0000],\n",
      "        [0.0544, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2103],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.2137]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0043, 0.1493],\n",
      "        [0.5200, 0.2270],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.3404, 0.5968],\n",
      "        [0.0000, 0.0841],\n",
      "        [0.0000, 0.2257]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0017, 0.0813],\n",
      "        [0.0000, 0.1167],\n",
      "        [0.0000, 0.1417],\n",
      "        ...,\n",
      "        [1.0121, 0.3030],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.4568]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.0030],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.4148],\n",
      "        ...,\n",
      "        [0.1888, 0.8639],\n",
      "        [0.0000, 0.0128],\n",
      "        [0.0000, 0.2259]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1037, 0.4459],\n",
      "        [0.2123, 0.0000],\n",
      "        [0.0000, 0.0349],\n",
      "        ...,\n",
      "        [0.4635, 0.5088],\n",
      "        [0.0613, 0.0000],\n",
      "        [0.1826, 0.1657]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3710, 0.1454],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.6078, 0.6148],\n",
      "        [0.2547, 0.0000],\n",
      "        [0.0000, 0.0310]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.3123],\n",
      "        [0.0237, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.2266, 0.0000],\n",
      "        [0.6085, 0.0000],\n",
      "        [0.0000, 0.3106]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.1988],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.0660, 0.6195],\n",
      "        [0.6724, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7936, 0.1143],\n",
      "        [0.0000, 0.2566],\n",
      "        [0.0000, 0.0141],\n",
      "        ...,\n",
      "        [0.3706, 0.6094],\n",
      "        [0.2491, 0.0237],\n",
      "        [0.0000, 0.1711]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5997, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0818, 0.0936],\n",
      "        ...,\n",
      "        [0.2024, 0.1310],\n",
      "        [0.0000, 0.3468],\n",
      "        [0.3840, 0.7559]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0423, 0.0000],\n",
      "        [0.2368, 0.0000],\n",
      "        [0.0000, 0.6243],\n",
      "        ...,\n",
      "        [0.7757, 1.1359],\n",
      "        [0.2297, 0.0000],\n",
      "        [0.0000, 0.6946]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3911, 0.2377],\n",
      "        [0.7038, 0.0000],\n",
      "        [0.0000, 0.1289],\n",
      "        ...,\n",
      "        [0.5053, 0.1659],\n",
      "        [0.2785, 0.0000],\n",
      "        [0.2818, 0.7353]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5382, 0.3441],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.7315, 0.4437],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1476, 0.1012],\n",
      "        [0.1409, 0.5672],\n",
      "        [0.0000, 1.2752],\n",
      "        ...,\n",
      "        [0.6287, 0.6339],\n",
      "        [0.0000, 0.0600],\n",
      "        [0.0000, 0.6177]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3590, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.7268],\n",
      "        ...,\n",
      "        [1.0448, 0.0000],\n",
      "        [0.2276, 0.0000],\n",
      "        [0.0000, 0.2043]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2854, 0.1409],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.6167, 0.6585],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.6247, 0.6098]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.1341],\n",
      "        [0.3618, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.3846],\n",
      "        [0.0767, 0.0268],\n",
      "        [0.0000, 1.3226]], grad_fn=<ReluBackward0>)\n",
      "epoch:  26  loss: 0.64809084\n",
      "tensor([[0.4297, 0.0654],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.2195],\n",
      "        ...,\n",
      "        [0.2937, 0.3427],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1427]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6032, 0.3204],\n",
      "        [0.2727, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.3782]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2465, 0.0000],\n",
      "        [0.0000, 0.1140],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4207],\n",
      "        [0.4687, 0.2953],\n",
      "        [0.0000, 0.1425]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2915, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0690],\n",
      "        ...,\n",
      "        [0.7242, 0.1711],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.4458]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3542, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.2741],\n",
      "        ...,\n",
      "        [0.8722, 0.3141],\n",
      "        [0.3487, 0.0000],\n",
      "        [0.0000, 0.5667]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8348, 0.1658],\n",
      "        [0.0000, 0.3710],\n",
      "        [0.0000, 0.2015],\n",
      "        ...,\n",
      "        [1.1679, 0.1463],\n",
      "        [0.5085, 0.0000],\n",
      "        [0.0000, 0.8892]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7909, 0.1605],\n",
      "        [0.0000, 0.5805],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2635],\n",
      "        [0.0615, 0.0000],\n",
      "        [0.2371, 0.1964]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2155, 0.0000],\n",
      "        [0.0000, 0.3259],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.4352, 0.2585],\n",
      "        [0.5348, 0.0000],\n",
      "        [0.0000, 0.8364]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0560, 0.0000],\n",
      "        [0.5856, 0.0000],\n",
      "        [0.0000, 0.8582],\n",
      "        ...,\n",
      "        [0.7269, 0.3449],\n",
      "        [0.0816, 0.0000],\n",
      "        [0.0000, 0.0681]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5981, 0.0297],\n",
      "        [0.0000, 0.3978],\n",
      "        [0.0000, 0.2702],\n",
      "        ...,\n",
      "        [1.4570, 1.2523],\n",
      "        [0.5462, 0.0000],\n",
      "        [0.0000, 0.6439]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4281, 0.0000],\n",
      "        [0.0000, 0.8169],\n",
      "        [0.0000, 0.2611],\n",
      "        ...,\n",
      "        [1.2544, 0.6981],\n",
      "        [0.9191, 0.0000],\n",
      "        [0.0000, 0.1362]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4822, 0.0000],\n",
      "        [0.4742, 0.0000],\n",
      "        [0.0000, 1.5675],\n",
      "        ...,\n",
      "        [0.2236, 0.4701],\n",
      "        [0.0000, 0.0772],\n",
      "        [0.0000, 0.4825]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6908, 0.0000],\n",
      "        [0.0000, 0.3397],\n",
      "        [0.0000, 0.9758],\n",
      "        ...,\n",
      "        [0.2530, 0.3823],\n",
      "        [0.0000, 0.1583],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1681, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.2661, 0.3811],\n",
      "        [1.6046, 0.0000],\n",
      "        [0.0000, 0.3609]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9762, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.2802],\n",
      "        ...,\n",
      "        [0.7741, 0.0000],\n",
      "        [0.5873, 0.0000],\n",
      "        [0.1496, 0.0137]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1435, 0.2007],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1035],\n",
      "        ...,\n",
      "        [0.6905, 0.3384],\n",
      "        [0.0238, 0.0000],\n",
      "        [0.0000, 0.3109]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8677, 0.0000],\n",
      "        [0.3824, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.0218, 0.2471],\n",
      "        [0.4241, 0.1737],\n",
      "        [0.0000, 0.3919]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1977, 0.3108],\n",
      "        [0.0000, 0.2062],\n",
      "        [0.0000, 0.0746],\n",
      "        ...,\n",
      "        [0.1696, 0.7366],\n",
      "        [0.2067, 0.0000],\n",
      "        [0.0000, 0.9507]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3981, 0.0000],\n",
      "        [0.0000, 0.4257],\n",
      "        [0.0000, 0.0228],\n",
      "        ...,\n",
      "        [0.9726, 0.1026],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1040]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1569, 0.0000],\n",
      "        [0.0000, 0.0513],\n",
      "        [0.0000, 0.6500],\n",
      "        ...,\n",
      "        [0.0000, 1.1900],\n",
      "        [1.0100, 0.0000],\n",
      "        [0.0000, 0.0834]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3192, 0.0000],\n",
      "        [0.1770, 0.0000],\n",
      "        [0.0000, 0.6540],\n",
      "        ...,\n",
      "        [0.7159, 0.1693],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1128]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1832, 0.0000],\n",
      "        [0.1588, 0.0000],\n",
      "        [0.0000, 0.7018],\n",
      "        ...,\n",
      "        [0.1055, 0.3455],\n",
      "        [0.0617, 0.0000],\n",
      "        [0.0000, 0.1656]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5731, 0.0000],\n",
      "        [0.0000, 0.5841],\n",
      "        [0.0000, 0.8882],\n",
      "        ...,\n",
      "        [0.4062, 0.2748],\n",
      "        [0.5944, 0.0000],\n",
      "        [0.0000, 0.0920]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4288, 0.0000],\n",
      "        [0.0000, 0.1288],\n",
      "        [0.0000, 0.9377],\n",
      "        ...,\n",
      "        [0.2239, 0.0000],\n",
      "        [1.4738, 0.0000],\n",
      "        [0.0000, 0.1594]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1246, 0.0000],\n",
      "        [0.0000, 0.1573],\n",
      "        [0.0000, 0.2138],\n",
      "        ...,\n",
      "        [0.8487, 0.1378],\n",
      "        [1.4894, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "epoch:  51  loss: 0.57897210\n",
      "tensor([[1.0380, 0.0000],\n",
      "        [0.3284, 0.0000],\n",
      "        [0.0000, 0.5919],\n",
      "        ...,\n",
      "        [0.4154, 0.1898],\n",
      "        [0.2101, 0.0000],\n",
      "        [0.0000, 0.2027]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4291, 0.0000],\n",
      "        [1.1513, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.6744, 0.3118],\n",
      "        [1.0306, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2542, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.9447],\n",
      "        ...,\n",
      "        [1.1567, 0.1721],\n",
      "        [0.7521, 0.0000],\n",
      "        [0.0000, 0.3946]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5921, 0.1537],\n",
      "        [0.1907, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.8380, 0.1220],\n",
      "        [0.0000, 0.4525],\n",
      "        [0.0113, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5501, 0.0000],\n",
      "        [0.9256, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.7063, 0.8704],\n",
      "        [0.5367, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.9766, 0.0983],\n",
      "        [0.0000, 0.1086],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1542, 0.2699],\n",
      "        [1.1314, 0.0000],\n",
      "        [0.4471, 0.0684]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8560, 0.2079],\n",
      "        [0.0573, 0.0000],\n",
      "        [0.0000, 0.2753],\n",
      "        ...,\n",
      "        [0.5634, 0.1873],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.3659]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2143, 0.0000],\n",
      "        [0.2184, 0.0241],\n",
      "        [0.0000, 0.1179],\n",
      "        ...,\n",
      "        [0.0000, 0.5943],\n",
      "        [1.4217, 0.0000],\n",
      "        [0.0000, 0.1109]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0434, 0.0385],\n",
      "        [0.9666, 0.0655],\n",
      "        [0.0000, 0.5739],\n",
      "        ...,\n",
      "        [1.1728, 0.9846],\n",
      "        [1.0622, 0.0000],\n",
      "        [0.0000, 0.2153]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9066, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.5255, 0.4252],\n",
      "        [0.0000, 0.1348],\n",
      "        [0.0000, 0.4494]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6112, 0.1054],\n",
      "        [0.3011, 0.0000],\n",
      "        [0.0000, 1.3206],\n",
      "        ...,\n",
      "        [1.4365, 0.7301],\n",
      "        [1.4822, 0.0000],\n",
      "        [0.0000, 0.0341]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6362, 0.2650],\n",
      "        [0.0000, 0.4683],\n",
      "        [0.0000, 1.1404],\n",
      "        ...,\n",
      "        [0.8023, 0.4628],\n",
      "        [0.0000, 0.0382],\n",
      "        [0.0000, 0.0356]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0418, 0.0528],\n",
      "        [0.0000, 0.4810],\n",
      "        [0.0000, 0.2601],\n",
      "        ...,\n",
      "        [0.0000, 0.1964],\n",
      "        [1.4367, 0.0000],\n",
      "        [0.0246, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6669, 0.0000],\n",
      "        [0.3938, 0.1855],\n",
      "        [0.0000, 0.7653],\n",
      "        ...,\n",
      "        [0.0000, 0.2307],\n",
      "        [2.7768, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.3425, 0.0000],\n",
      "        [0.0000, 0.5610],\n",
      "        [0.0000, 0.6013],\n",
      "        ...,\n",
      "        [1.1809, 0.3313],\n",
      "        [1.0669, 0.0000],\n",
      "        [0.0000, 0.4428]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0896, 0.1303],\n",
      "        [0.4092, 0.0000],\n",
      "        [0.0000, 1.1627],\n",
      "        ...,\n",
      "        [0.6489, 0.3838],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.2593]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1457, 0.0000],\n",
      "        [0.0000, 0.4130],\n",
      "        [0.0000, 0.4273],\n",
      "        ...,\n",
      "        [0.8946, 0.2597],\n",
      "        [0.4641, 0.0000],\n",
      "        [0.9324, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7846, 0.0000],\n",
      "        [0.0340, 0.0000],\n",
      "        [0.0000, 1.4501],\n",
      "        ...,\n",
      "        [0.1703, 0.3441],\n",
      "        [0.6870, 0.0000],\n",
      "        [0.0000, 0.3147]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2856, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.6771],\n",
      "        ...,\n",
      "        [1.0868, 0.0000],\n",
      "        [1.1191, 0.0000],\n",
      "        [0.0000, 0.1380]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9105, 0.0000],\n",
      "        [0.0000, 0.4475],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.7394, 0.4225],\n",
      "        [2.3970, 0.0000],\n",
      "        [0.0000, 0.2501]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1102, 0.0000],\n",
      "        [0.3213, 0.1061],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0862, 0.1643],\n",
      "        [0.0540, 0.0000],\n",
      "        [0.0000, 0.1166]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.7144, 0.0000],\n",
      "        [0.0000, 0.4030],\n",
      "        [0.0000, 1.2105],\n",
      "        ...,\n",
      "        [0.4743, 0.4359],\n",
      "        [2.9884, 0.0000],\n",
      "        [0.0000, 0.3113]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3154, 0.1526],\n",
      "        [0.4136, 0.0000],\n",
      "        [0.0000, 0.6428],\n",
      "        ...,\n",
      "        [0.6110, 0.1730],\n",
      "        [1.0051, 0.0000],\n",
      "        [0.0000, 0.2848]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8920, 0.0315],\n",
      "        [0.0000, 0.0825],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.4130, 0.1676],\n",
      "        [1.4671, 0.0000],\n",
      "        [0.0000, 0.2358]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.9979, 0.5039],\n",
      "        [0.9307, 0.0000],\n",
      "        [0.0000, 1.1800],\n",
      "        ...,\n",
      "        [0.0000, 0.5539],\n",
      "        [1.6528, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "epoch:  76  loss: 0.52367818\n",
      "tensor([[1.6341, 0.0000],\n",
      "        [0.0000, 0.1666],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.8001, 0.0213],\n",
      "        [1.5773, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7929, 0.1076],\n",
      "        [0.0000, 0.3642],\n",
      "        [0.0000, 0.2432],\n",
      "        ...,\n",
      "        [0.0829, 0.1876],\n",
      "        [1.2076, 0.0000],\n",
      "        [0.0000, 0.2473]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2913, 0.0000],\n",
      "        [0.0000, 0.0305],\n",
      "        [0.0000, 0.3747],\n",
      "        ...,\n",
      "        [0.0101, 0.1068],\n",
      "        [1.0590, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0185, 0.0000],\n",
      "        [0.6941, 0.0000],\n",
      "        [0.0000, 0.6791],\n",
      "        ...,\n",
      "        [0.8403, 0.6976],\n",
      "        [1.4437, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3295, 0.0109],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.7995],\n",
      "        ...,\n",
      "        [0.7625, 0.6211],\n",
      "        [1.7581, 0.0000],\n",
      "        [0.0000, 0.1803]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3982, 0.0852],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.5120],\n",
      "        ...,\n",
      "        [0.0325, 0.1092],\n",
      "        [0.8764, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2241, 0.0000],\n",
      "        [0.2196, 0.0098],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4134],\n",
      "        [1.3463, 0.0000],\n",
      "        [0.0000, 0.9425]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2098, 0.1852],\n",
      "        [0.5029, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.8125, 0.1983],\n",
      "        [2.1517, 0.0000],\n",
      "        [0.0000, 0.2311]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9260, 0.0000],\n",
      "        [0.4773, 0.0000],\n",
      "        [0.0000, 0.2293],\n",
      "        ...,\n",
      "        [0.0000, 0.9803],\n",
      "        [1.0351, 0.0000],\n",
      "        [0.0000, 0.2412]], grad_fn=<ReluBackward0>)\n",
      "tensor([[3.0406, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.6787],\n",
      "        ...,\n",
      "        [0.5101, 0.4127],\n",
      "        [0.8409, 0.0000],\n",
      "        [0.0000, 0.3653]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7842, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1647],\n",
      "        ...,\n",
      "        [0.6929, 1.1168],\n",
      "        [1.8088, 0.0000],\n",
      "        [0.2660, 0.0451]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.5945, 0.0000],\n",
      "        [1.0603, 0.0000],\n",
      "        [0.0000, 0.3208],\n",
      "        ...,\n",
      "        [0.5007, 0.2217],\n",
      "        [0.5503, 0.0000],\n",
      "        [0.0000, 0.1369]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8136, 0.0251],\n",
      "        [0.0000, 0.5571],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.3875],\n",
      "        [0.9112, 0.0000],\n",
      "        [0.0000, 0.0435]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5515, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.4871],\n",
      "        ...,\n",
      "        [0.0000, 0.5733],\n",
      "        [0.6943, 0.0000],\n",
      "        [0.3194, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6111, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.6284],\n",
      "        ...,\n",
      "        [0.0000, 0.3954],\n",
      "        [0.5865, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9718, 0.0000],\n",
      "        [0.0000, 0.0629],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4516],\n",
      "        [2.0507, 0.0000],\n",
      "        [0.0000, 0.0251]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7997, 0.2119],\n",
      "        [0.0000, 0.0277],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.9037],\n",
      "        [2.7723, 0.0000],\n",
      "        [0.2185, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9764, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.9585],\n",
      "        ...,\n",
      "        [0.0133, 0.4205],\n",
      "        [3.0265, 0.0000],\n",
      "        [0.0000, 0.8075]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8108, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 1.0690],\n",
      "        ...,\n",
      "        [0.0396, 0.4203],\n",
      "        [1.5002, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2040, 0.1283],\n",
      "        [0.0000, 0.3210],\n",
      "        [0.0000, 1.1154],\n",
      "        ...,\n",
      "        [0.8994, 0.1427],\n",
      "        [2.0185, 0.0000],\n",
      "        [0.0000, 0.1940]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.9612, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1724],\n",
      "        ...,\n",
      "        [0.0000, 0.4517],\n",
      "        [1.0684, 0.0000],\n",
      "        [0.0000, 0.3074]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1889, 0.0000],\n",
      "        [0.0000, 0.2858],\n",
      "        [0.1874, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5053],\n",
      "        [1.9351, 0.0000],\n",
      "        [0.0000, 0.0978]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1001, 0.0000],\n",
      "        [0.0000, 0.2313],\n",
      "        [0.0000, 1.2233],\n",
      "        ...,\n",
      "        [0.3753, 0.3900],\n",
      "        [1.0017, 0.0000],\n",
      "        [0.0000, 0.2069]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7852, 0.0000],\n",
      "        [1.1186, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.2404, 0.1629],\n",
      "        [2.4794, 0.0000],\n",
      "        [0.0000, 0.1702]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3736, 0.0000],\n",
      "        [0.0000, 0.0638],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.5696, 0.4050],\n",
      "        [2.2976, 0.0000],\n",
      "        [0.0000, 0.1189]], grad_fn=<ReluBackward0>)\n",
      "epoch: 101  loss: 0.48471227\n",
      "tensor([[1.4148, 0.0328],\n",
      "        [0.0000, 0.1315],\n",
      "        [0.0000, 1.1787],\n",
      "        ...,\n",
      "        [0.2184, 0.1941],\n",
      "        [2.6100, 0.0000],\n",
      "        [0.0000, 0.2032]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3421, 0.0000],\n",
      "        [0.0000, 0.1598],\n",
      "        [0.0000, 0.4442],\n",
      "        ...,\n",
      "        [0.0000, 0.9422],\n",
      "        [1.0377, 0.0000],\n",
      "        [0.0000, 0.6107]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0511, 0.0000],\n",
      "        [0.2008, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5096],\n",
      "        [0.9548, 0.0000],\n",
      "        [0.0000, 0.7211]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5335, 0.0000],\n",
      "        [0.0000, 0.2777],\n",
      "        [0.0000, 0.2837],\n",
      "        ...,\n",
      "        [0.0000, 1.0055],\n",
      "        [2.6740, 0.0000],\n",
      "        [0.0000, 0.1002]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4259, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.3535],\n",
      "        ...,\n",
      "        [0.0000, 1.2766],\n",
      "        [2.4496, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2706, 0.0807],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1439],\n",
      "        ...,\n",
      "        [0.0000, 0.4231],\n",
      "        [1.1437, 0.0000],\n",
      "        [0.0000, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3022, 0.0000],\n",
      "        [0.8277, 0.0000],\n",
      "        [0.0000, 0.2583],\n",
      "        ...,\n",
      "        [0.0000, 0.2743],\n",
      "        [2.8587, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9304, 0.2891],\n",
      "        [1.3918, 0.0000],\n",
      "        [0.0000, 1.7473],\n",
      "        ...,\n",
      "        [0.8565, 0.5545],\n",
      "        [0.3930, 0.0000],\n",
      "        [0.0000, 0.1758]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0994, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0239],\n",
      "        ...,\n",
      "        [0.6614, 0.5065],\n",
      "        [1.2421, 0.0000],\n",
      "        [0.0000, 0.7742]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0759, 0.0214],\n",
      "        [0.0000, 0.0190],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2551],\n",
      "        [0.9944, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9800, 0.0000],\n",
      "        [0.2937, 0.1414],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.2271, 0.5070],\n",
      "        [2.1855, 0.0000],\n",
      "        [0.0000, 0.8053]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5022, 0.2051],\n",
      "        [0.0000, 0.0946],\n",
      "        [0.0000, 0.3200],\n",
      "        ...,\n",
      "        [0.0000, 0.3379],\n",
      "        [3.2296, 0.0000],\n",
      "        [0.0000, 0.7880]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0235, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.5621],\n",
      "        ...,\n",
      "        [0.3407, 0.5249],\n",
      "        [3.3895, 0.0000],\n",
      "        [0.0000, 0.9984]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6323, 0.0000],\n",
      "        [0.5509, 0.0000],\n",
      "        [0.0788, 0.0718],\n",
      "        ...,\n",
      "        [0.0000, 0.3754],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0685]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1431, 0.0000],\n",
      "        [0.0000, 0.2876],\n",
      "        [0.0000, 0.4793],\n",
      "        ...,\n",
      "        [0.0000, 0.3611],\n",
      "        [3.3398, 0.0000],\n",
      "        [0.0000, 0.5142]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3094, 0.0000],\n",
      "        [0.3235, 0.0000],\n",
      "        [0.0000, 0.7160],\n",
      "        ...,\n",
      "        [0.0000, 0.3910],\n",
      "        [2.0074, 0.0000],\n",
      "        [0.0000, 0.5239]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2848, 0.0000],\n",
      "        [0.0000, 0.0033],\n",
      "        [0.0000, 0.2212],\n",
      "        ...,\n",
      "        [0.0000, 1.1190],\n",
      "        [0.4397, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3847, 0.0000],\n",
      "        [0.0000, 0.2280],\n",
      "        [0.0000, 0.6982],\n",
      "        ...,\n",
      "        [0.0000, 1.1064],\n",
      "        [0.8958, 0.0000],\n",
      "        [0.0000, 0.0368]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8382, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.3266],\n",
      "        ...,\n",
      "        [0.0000, 0.1624],\n",
      "        [0.7980, 0.0000],\n",
      "        [0.0000, 0.2133]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6707, 0.0000],\n",
      "        [0.0000, 0.1945],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.5582, 0.3707],\n",
      "        [1.1741, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2704, 0.0000],\n",
      "        [0.0435, 0.0000],\n",
      "        [1.4569, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.2157],\n",
      "        [3.6650, 0.0000],\n",
      "        [0.0000, 0.2099]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1843, 0.0000],\n",
      "        [0.0000, 0.1667],\n",
      "        [0.0000, 0.6107],\n",
      "        ...,\n",
      "        [0.4895, 0.0832],\n",
      "        [3.0346, 0.0000],\n",
      "        [0.0000, 0.0817]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7104, 0.0000],\n",
      "        [0.0000, 0.5027],\n",
      "        [2.0330, 0.0000],\n",
      "        ...,\n",
      "        [0.1659, 0.4361],\n",
      "        [1.4514, 0.0000],\n",
      "        [0.0000, 0.5126]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6709, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0706],\n",
      "        ...,\n",
      "        [0.3577, 0.2356],\n",
      "        [2.7409, 0.0000],\n",
      "        [0.0000, 0.0222]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9785, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.9388],\n",
      "        ...,\n",
      "        [0.0000, 0.1083],\n",
      "        [1.4097, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "epoch: 126  loss: 0.45659387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1469, 0.0000],\n",
      "        [0.8171, 0.0000],\n",
      "        [0.0000, 0.1996],\n",
      "        ...,\n",
      "        [0.2311, 0.3317],\n",
      "        [1.7534, 0.0000],\n",
      "        [0.0000, 1.0304]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4327, 0.0000],\n",
      "        [0.4933, 0.0000],\n",
      "        [0.0000, 0.2114],\n",
      "        ...,\n",
      "        [0.0000, 0.6641],\n",
      "        [2.6348, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5659, 0.0000],\n",
      "        [0.3766, 0.0790],\n",
      "        [0.0000, 0.6185],\n",
      "        ...,\n",
      "        [0.0304, 0.0850],\n",
      "        [1.4803, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.3371, 0.0000],\n",
      "        [0.0000, 0.2462],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.3919],\n",
      "        [2.7939, 0.0000],\n",
      "        [0.0000, 0.8519]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.3540, 0.0000],\n",
      "        [0.7392, 0.0000],\n",
      "        [0.0000, 2.1899],\n",
      "        ...,\n",
      "        [0.0000, 0.5636],\n",
      "        [2.3187, 0.0000],\n",
      "        [0.0000, 0.5401]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1380, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.4217],\n",
      "        ...,\n",
      "        [0.0000, 0.9664],\n",
      "        [3.9634, 0.0000],\n",
      "        [0.0000, 0.1525]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8153, 0.0000],\n",
      "        [1.1378, 0.0000],\n",
      "        [0.0000, 0.4504],\n",
      "        ...,\n",
      "        [0.0000, 0.2441],\n",
      "        [2.5383, 0.0000],\n",
      "        [0.0000, 0.1811]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6589, 0.0938],\n",
      "        [0.1388, 0.4415],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.8635],\n",
      "        [2.2925, 0.0000],\n",
      "        [0.0000, 0.0168]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.5669, 0.0000],\n",
      "        [0.3207, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2620],\n",
      "        [2.9685, 0.0000],\n",
      "        [0.0000, 0.3127]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0336, 0.0000],\n",
      "        [0.0000, 0.4686],\n",
      "        [0.0000, 0.4389],\n",
      "        ...,\n",
      "        [0.0000, 0.3335],\n",
      "        [2.7614, 0.0000],\n",
      "        [0.0000, 0.2829]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1005, 0.0000],\n",
      "        [0.0000, 0.5756],\n",
      "        [0.0000, 0.0750],\n",
      "        ...,\n",
      "        [0.0000, 0.5019],\n",
      "        [2.9921, 0.0000],\n",
      "        [0.0000, 0.8081]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1157, 0.0000],\n",
      "        [0.7021, 0.0000],\n",
      "        [0.0000, 0.0688],\n",
      "        ...,\n",
      "        [0.7243, 0.3432],\n",
      "        [0.5822, 0.0000],\n",
      "        [0.0000, 0.5558]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8287, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 1.3267],\n",
      "        ...,\n",
      "        [0.0000, 0.5310],\n",
      "        [0.7552, 0.0000],\n",
      "        [0.0000, 0.6261]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2160, 0.0000],\n",
      "        [0.0000, 0.2887],\n",
      "        [0.0000, 0.4372],\n",
      "        ...,\n",
      "        [0.0000, 1.1306],\n",
      "        [2.3730, 0.0000],\n",
      "        [0.0000, 0.1312]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5235, 0.0000],\n",
      "        [0.5530, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0927],\n",
      "        [1.9733, 0.0000],\n",
      "        [0.0000, 0.3767]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4316, 0.0000],\n",
      "        [0.0000, 0.2157],\n",
      "        [0.0000, 0.4254],\n",
      "        ...,\n",
      "        [0.0000, 0.3616],\n",
      "        [3.1498, 0.0000],\n",
      "        [0.0000, 0.3251]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8333, 0.0671],\n",
      "        [1.1276, 0.0000],\n",
      "        [0.0000, 0.6505],\n",
      "        ...,\n",
      "        [0.0000, 0.9748],\n",
      "        [2.7616, 0.0000],\n",
      "        [0.0000, 0.3983]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1894, 0.0000],\n",
      "        [0.0783, 0.0203],\n",
      "        [0.0000, 2.1129],\n",
      "        ...,\n",
      "        [0.0703, 0.4349],\n",
      "        [1.3682, 0.0000],\n",
      "        [0.0000, 0.0748]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7959, 0.0000],\n",
      "        [1.1168, 0.0000],\n",
      "        [0.0000, 0.0991],\n",
      "        ...,\n",
      "        [0.2137, 0.4627],\n",
      "        [2.7444, 0.0000],\n",
      "        [0.0000, 0.5539]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.6252, 0.0000],\n",
      "        [0.0000, 0.0078],\n",
      "        [0.3978, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2447],\n",
      "        [2.5494, 0.0000],\n",
      "        [0.0000, 1.1023]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6310, 0.0000],\n",
      "        [0.0000, 0.5331],\n",
      "        [0.0000, 0.3517],\n",
      "        ...,\n",
      "        [0.0000, 0.8645],\n",
      "        [3.6959, 0.0000],\n",
      "        [0.0000, 0.5647]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0693, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [2.2750, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.1061],\n",
      "        [2.3523, 0.0000],\n",
      "        [0.0000, 0.9760]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8248, 0.0000],\n",
      "        [0.0375, 0.0000],\n",
      "        [0.0000, 1.4803],\n",
      "        ...,\n",
      "        [0.0000, 0.8654],\n",
      "        [1.8624, 0.0000],\n",
      "        [0.0000, 0.0996]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8092, 0.0392],\n",
      "        [0.0000, 0.0906],\n",
      "        [0.0000, 0.5576],\n",
      "        ...,\n",
      "        [0.3124, 0.4987],\n",
      "        [1.4313, 0.0000],\n",
      "        [0.0000, 0.9845]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2322, 0.0592],\n",
      "        [0.0000, 0.6267],\n",
      "        [0.0000, 0.1531],\n",
      "        ...,\n",
      "        [0.0000, 0.4499],\n",
      "        [0.4432, 0.0000],\n",
      "        [0.0000, 1.0887]], grad_fn=<ReluBackward0>)\n",
      "epoch: 151  loss: 0.43487591\n",
      "tensor([[2.7080, 0.0000],\n",
      "        [0.0000, 0.2999],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.2851, 0.2357],\n",
      "        [3.1944, 0.0000],\n",
      "        [0.0000, 1.0012]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5132, 0.0000],\n",
      "        [1.2228, 0.0000],\n",
      "        [0.0000, 1.5565],\n",
      "        ...,\n",
      "        [0.0000, 0.9783],\n",
      "        [2.8007, 0.0000],\n",
      "        [0.0000, 0.6756]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.7292, 0.0000],\n",
      "        [0.3242, 0.2305],\n",
      "        [0.0000, 0.0166],\n",
      "        ...,\n",
      "        [0.0000, 0.4438],\n",
      "        [3.4556, 0.0000],\n",
      "        [0.0000, 0.6127]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.9121, 0.0000],\n",
      "        [0.0000, 0.4706],\n",
      "        [0.0000, 1.6550],\n",
      "        ...,\n",
      "        [0.0000, 0.7099],\n",
      "        [1.6090, 0.0000],\n",
      "        [0.0000, 0.7290]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1027, 0.0954],\n",
      "        [0.1814, 0.0000],\n",
      "        [0.0000, 1.1036],\n",
      "        ...,\n",
      "        [0.0000, 0.6663],\n",
      "        [2.2169, 0.0000],\n",
      "        [0.0000, 0.1748]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2108, 0.0263],\n",
      "        [0.8072, 0.0722],\n",
      "        [0.0000, 0.1406],\n",
      "        ...,\n",
      "        [0.0000, 0.9217],\n",
      "        [2.2125, 0.0000],\n",
      "        [0.0000, 0.3144]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0170, 0.3060],\n",
      "        [0.0000, 0.2974],\n",
      "        [0.0000, 0.5613],\n",
      "        ...,\n",
      "        [0.0000, 1.0374],\n",
      "        [2.5846, 0.0000],\n",
      "        [0.0000, 0.9762]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0843, 0.0000],\n",
      "        [0.0000, 0.4836],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4932],\n",
      "        [1.5668, 0.0000],\n",
      "        [0.0000, 0.1761]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7184, 0.0000],\n",
      "        [0.0000, 0.3606],\n",
      "        [0.0000, 0.8674],\n",
      "        ...,\n",
      "        [0.0000, 0.2999],\n",
      "        [3.2580, 0.0000],\n",
      "        [0.0000, 0.4209]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1734, 0.0000],\n",
      "        [0.0000, 0.3119],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.0906],\n",
      "        [2.0270, 0.0000],\n",
      "        [0.0000, 0.7019]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.5191, 0.0000],\n",
      "        [0.0000, 0.3689],\n",
      "        [0.0000, 1.0667],\n",
      "        ...,\n",
      "        [0.0000, 0.4693],\n",
      "        [3.2295, 0.0000],\n",
      "        [0.0000, 0.1617]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1974, 0.0000],\n",
      "        [0.0000, 0.3708],\n",
      "        [0.0000, 0.6952],\n",
      "        ...,\n",
      "        [0.0000, 0.2673],\n",
      "        [2.1905, 0.0000],\n",
      "        [0.0000, 1.0290]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7859, 0.0433],\n",
      "        [0.8376, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6127],\n",
      "        [1.3051, 0.0000],\n",
      "        [0.0000, 0.3117]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7438, 0.0000],\n",
      "        [0.1935, 0.2818],\n",
      "        [0.0000, 0.2343],\n",
      "        ...,\n",
      "        [0.0000, 0.5765],\n",
      "        [2.2596, 0.0000],\n",
      "        [0.0000, 0.2664]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4859, 0.0000],\n",
      "        [0.0000, 0.0607],\n",
      "        [0.7016, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.8831],\n",
      "        [0.9057, 0.0000],\n",
      "        [0.0000, 0.6415]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6478, 0.0000],\n",
      "        [0.0000, 0.5015],\n",
      "        [0.0000, 1.1350],\n",
      "        ...,\n",
      "        [0.0000, 0.9803],\n",
      "        [3.4006, 0.0000],\n",
      "        [0.0000, 0.9131]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6890, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.5046],\n",
      "        ...,\n",
      "        [0.0000, 0.5978],\n",
      "        [1.5450, 0.0000],\n",
      "        [0.0000, 0.0774]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8378, 0.0000],\n",
      "        [0.0000, 0.0941],\n",
      "        [0.0000, 0.6866],\n",
      "        ...,\n",
      "        [0.2567, 0.3132],\n",
      "        [2.4426, 0.0000],\n",
      "        [0.0000, 0.1756]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4530, 0.0000],\n",
      "        [1.2347, 0.0000],\n",
      "        [0.0000, 0.4941],\n",
      "        ...,\n",
      "        [0.0000, 0.7087],\n",
      "        [1.9645, 0.0000],\n",
      "        [0.0000, 0.2050]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8062, 0.0000],\n",
      "        [0.5014, 0.0000],\n",
      "        [0.1658, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.8493],\n",
      "        [2.3333, 0.0000],\n",
      "        [0.0000, 0.8378]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7368, 0.0000],\n",
      "        [0.2092, 0.5203],\n",
      "        [0.0567, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5926],\n",
      "        [1.1344, 0.0900],\n",
      "        [0.0000, 0.7316]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1523, 0.0000],\n",
      "        [0.0000, 0.1297],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.7144],\n",
      "        [0.7001, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5211, 0.1329],\n",
      "        [0.0000, 0.2846],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.2688],\n",
      "        [1.3087, 0.0000],\n",
      "        [0.0000, 1.0413]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.3033, 0.0000],\n",
      "        [0.0000, 0.1346],\n",
      "        [0.0000, 0.5686],\n",
      "        ...,\n",
      "        [0.0000, 1.2261],\n",
      "        [1.9726, 0.0000],\n",
      "        [0.0000, 0.2344]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1983, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.7037, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.4833],\n",
      "        [2.0737, 0.0000],\n",
      "        [0.0000, 0.3100]], grad_fn=<ReluBackward0>)\n",
      "epoch: 176  loss: 0.41419038\n",
      "tensor([[1.8772, 0.0000],\n",
      "        [0.8372, 0.4009],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.1655],\n",
      "        [2.8795, 0.0000],\n",
      "        [0.0000, 0.8197]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.6404, 0.0000],\n",
      "        [0.4625, 0.0000],\n",
      "        [0.0000, 0.9076],\n",
      "        ...,\n",
      "        [0.0000, 1.3340],\n",
      "        [1.5921, 0.0000],\n",
      "        [0.0000, 1.2639]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4612, 0.0000],\n",
      "        [0.0000, 0.3034],\n",
      "        [0.0000, 0.9415],\n",
      "        ...,\n",
      "        [0.0000, 0.9565],\n",
      "        [1.3142, 0.0000],\n",
      "        [0.0000, 0.1182]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8766, 0.0000],\n",
      "        [0.3999, 0.0458],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1004, 0.5600],\n",
      "        [1.2707, 0.0000],\n",
      "        [0.0000, 1.1409]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8085, 0.0000],\n",
      "        [0.0000, 0.3278],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.1507],\n",
      "        [2.6542, 0.0000],\n",
      "        [0.0000, 0.0519]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5953, 0.0000],\n",
      "        [0.7412, 0.0000],\n",
      "        [0.0000, 0.0447],\n",
      "        ...,\n",
      "        [0.0000, 0.3661],\n",
      "        [1.5326, 0.0000],\n",
      "        [0.0000, 0.4666]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0401, 0.0000],\n",
      "        [0.0022, 0.0000],\n",
      "        [0.0000, 0.7167],\n",
      "        ...,\n",
      "        [0.0000, 0.9550],\n",
      "        [0.7218, 0.0000],\n",
      "        [0.0000, 0.0942]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.9172, 0.0000],\n",
      "        [0.9464, 0.0000],\n",
      "        [0.0000, 1.5041],\n",
      "        ...,\n",
      "        [0.0000, 0.6633],\n",
      "        [4.0086, 0.0000],\n",
      "        [0.0000, 1.0554]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.3276, 0.0000],\n",
      "        [0.0000, 0.7131],\n",
      "        [0.0000, 1.1444],\n",
      "        ...,\n",
      "        [0.0000, 0.4667],\n",
      "        [2.7067, 0.0000],\n",
      "        [0.0000, 0.6644]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4563, 0.0000],\n",
      "        [0.0000, 0.4257],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5168],\n",
      "        [1.1323, 0.0000],\n",
      "        [0.0000, 0.5408]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.3070, 0.0000],\n",
      "        [0.0000, 0.1501],\n",
      "        [0.0000, 1.1978],\n",
      "        ...,\n",
      "        [0.0000, 0.7639],\n",
      "        [3.6990, 0.0000],\n",
      "        [0.0000, 1.3318]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2480, 0.0000],\n",
      "        [0.0000, 0.7491],\n",
      "        [0.0000, 0.0143],\n",
      "        ...,\n",
      "        [0.0000, 0.7710],\n",
      "        [1.7964, 0.0000],\n",
      "        [0.0000, 0.5335]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3852, 0.0000],\n",
      "        [0.0000, 0.2033],\n",
      "        [0.0000, 0.8700],\n",
      "        ...,\n",
      "        [0.0000, 0.4177],\n",
      "        [3.1891, 0.0000],\n",
      "        [0.0000, 0.1559]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1527, 0.0000],\n",
      "        [0.0000, 0.2370],\n",
      "        [1.1021, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.1456],\n",
      "        [2.1711, 0.0000],\n",
      "        [0.0000, 0.9902]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2530e+00, 0.0000e+00],\n",
      "        [5.1183e-01, 2.2184e-03],\n",
      "        [0.0000e+00, 2.0805e-01],\n",
      "        ...,\n",
      "        [0.0000e+00, 2.9283e-01],\n",
      "        [2.9702e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1625e+00]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5907, 0.0000],\n",
      "        [0.5638, 0.0436],\n",
      "        [0.0000, 0.1572],\n",
      "        ...,\n",
      "        [0.0000, 0.7218],\n",
      "        [1.0692, 0.0000],\n",
      "        [0.0000, 1.1485]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4860, 0.0000],\n",
      "        [0.0000, 0.0932],\n",
      "        [0.0000, 0.0132],\n",
      "        ...,\n",
      "        [0.0000, 1.2865],\n",
      "        [2.8743, 0.0000],\n",
      "        [0.0000, 1.3692]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2366, 0.0000],\n",
      "        [0.3790, 0.0000],\n",
      "        [1.3039, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4661],\n",
      "        [2.6987, 0.0000],\n",
      "        [0.0000, 0.8525]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7857, 0.0000],\n",
      "        [0.0000, 0.7906],\n",
      "        [0.0000, 0.6628],\n",
      "        ...,\n",
      "        [0.0000, 0.9145],\n",
      "        [2.7176, 0.0000],\n",
      "        [0.0000, 1.2607]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3759, 0.0000],\n",
      "        [0.0000, 0.2360],\n",
      "        [1.3470, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.8808],\n",
      "        [3.3456, 0.0000],\n",
      "        [0.0000, 0.9295]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1632, 0.0000],\n",
      "        [0.0000, 0.5731],\n",
      "        [0.0000, 1.1919],\n",
      "        ...,\n",
      "        [0.0000, 0.7107],\n",
      "        [1.2074, 0.0000],\n",
      "        [0.0000, 1.5907]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4964, 0.0000],\n",
      "        [0.0000, 0.7674],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6320],\n",
      "        [2.9466, 0.0000],\n",
      "        [0.0000, 0.8352]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.6261, 0.0000],\n",
      "        [0.0000, 0.1752],\n",
      "        [0.0000, 0.0604],\n",
      "        ...,\n",
      "        [0.0000, 0.6867],\n",
      "        [3.7813, 0.0000],\n",
      "        [0.0000, 0.9861]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4105, 0.0000],\n",
      "        [0.0000, 0.2797],\n",
      "        [0.0000, 0.5020],\n",
      "        ...,\n",
      "        [0.0000, 0.2965],\n",
      "        [2.7050, 0.0000],\n",
      "        [0.0000, 0.5880]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8782, 0.0000],\n",
      "        [0.0000, 0.3527],\n",
      "        [0.0000, 0.4020],\n",
      "        ...,\n",
      "        [0.0000, 1.4005],\n",
      "        [2.6788, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "epoch: 201  loss: 0.40610978\n",
      "tensor([[1.1385, 0.0000],\n",
      "        [0.0000, 0.3774],\n",
      "        [0.0000, 0.3104],\n",
      "        ...,\n",
      "        [0.0000, 0.6089],\n",
      "        [3.1142, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.5199, 0.0000],\n",
      "        [0.0000, 0.5918],\n",
      "        [0.0000, 0.3705],\n",
      "        ...,\n",
      "        [0.0000, 0.2336],\n",
      "        [2.7210, 0.0000],\n",
      "        [0.0000, 0.6616]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2953, 0.0000],\n",
      "        [0.0000, 0.2603],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5895],\n",
      "        [4.1178, 0.0000],\n",
      "        [0.0000, 1.6407]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4849, 0.0000],\n",
      "        [0.9800, 0.0000],\n",
      "        [0.0000, 0.1917],\n",
      "        ...,\n",
      "        [0.0000, 0.4079],\n",
      "        [2.2559, 0.0000],\n",
      "        [0.0000, 1.1116]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1395, 0.0000],\n",
      "        [0.5374, 0.0589],\n",
      "        [0.0000, 1.1174],\n",
      "        ...,\n",
      "        [0.0000, 0.7171],\n",
      "        [2.8385, 0.0000],\n",
      "        [0.0000, 1.0867]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4866, 0.0000],\n",
      "        [0.3846, 0.3055],\n",
      "        [0.0000, 1.1200],\n",
      "        ...,\n",
      "        [0.0000, 0.5862],\n",
      "        [0.0000, 0.2139],\n",
      "        [0.0000, 0.9598]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.9727, 0.0000],\n",
      "        [0.0000, 0.3419],\n",
      "        [0.0000, 1.0840],\n",
      "        ...,\n",
      "        [0.0000, 0.7278],\n",
      "        [1.5046, 0.0000],\n",
      "        [0.0000, 0.9386]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5377, 0.0000],\n",
      "        [0.0000, 0.9652],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5266],\n",
      "        [3.5235, 0.0000],\n",
      "        [0.0000, 0.7074]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3779, 0.0000],\n",
      "        [0.0000, 0.3094],\n",
      "        [0.0000, 0.4014],\n",
      "        ...,\n",
      "        [0.0000, 1.0726],\n",
      "        [2.7291, 0.0000],\n",
      "        [0.0000, 0.7593]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2724, 0.0000],\n",
      "        [0.4033, 0.2621],\n",
      "        [0.0000, 0.0541],\n",
      "        ...,\n",
      "        [0.0000, 0.6988],\n",
      "        [1.8840, 0.0000],\n",
      "        [0.0000, 1.3096]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.9768, 0.0000],\n",
      "        [0.0000, 0.3933],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.3214],\n",
      "        [2.8873, 0.0000],\n",
      "        [0.0000, 1.0864]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1097, 0.0000],\n",
      "        [0.1091, 0.3255],\n",
      "        [0.0000, 1.4118],\n",
      "        ...,\n",
      "        [0.0000, 0.2300],\n",
      "        [3.1619, 0.0000],\n",
      "        [0.0000, 0.8907]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.7544, 0.0000],\n",
      "        [0.0000, 0.3052],\n",
      "        [0.0000, 0.3771],\n",
      "        ...,\n",
      "        [0.0000, 0.6690],\n",
      "        [0.8927, 0.0000],\n",
      "        [0.0000, 0.9511]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0318, 0.0000],\n",
      "        [0.0000, 0.3880],\n",
      "        [2.0924, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4280],\n",
      "        [0.6650, 0.0000],\n",
      "        [0.0000, 0.5711]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3302, 0.0000],\n",
      "        [0.0000, 0.5949],\n",
      "        [0.0000, 0.3332],\n",
      "        ...,\n",
      "        [0.0000, 0.6182],\n",
      "        [2.3281, 0.0000],\n",
      "        [0.0000, 0.2568]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8022, 0.0000],\n",
      "        [0.4133, 0.3045],\n",
      "        [0.0000, 1.2635],\n",
      "        ...,\n",
      "        [0.0000, 0.3144],\n",
      "        [1.3188, 0.0000],\n",
      "        [0.0000, 0.4275]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3708, 0.0000],\n",
      "        [0.0000, 0.8484],\n",
      "        [0.0000, 0.7166],\n",
      "        ...,\n",
      "        [0.0000, 0.7063],\n",
      "        [1.3036, 0.0000],\n",
      "        [0.0000, 0.6880]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5156, 0.0000],\n",
      "        [0.1449, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.3084],\n",
      "        [1.3362, 0.0102],\n",
      "        [0.0000, 0.1160]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1624, 0.0000],\n",
      "        [0.0164, 0.2398],\n",
      "        [0.0000, 0.2971],\n",
      "        ...,\n",
      "        [0.0000, 1.2093],\n",
      "        [1.0466, 0.0000],\n",
      "        [0.0000, 1.0708]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8059, 0.0000],\n",
      "        [0.4895, 0.1652],\n",
      "        [0.0000, 0.7749],\n",
      "        ...,\n",
      "        [0.0000, 0.7020],\n",
      "        [1.2564, 0.0000],\n",
      "        [0.0000, 1.2952]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5558, 0.0000],\n",
      "        [0.0000, 0.6413],\n",
      "        [0.0000, 0.4911],\n",
      "        ...,\n",
      "        [0.0000, 0.6539],\n",
      "        [2.2093, 0.0000],\n",
      "        [0.0000, 0.5947]], grad_fn=<ReluBackward0>)\n",
      "tensor([[3.1090, 0.0000],\n",
      "        [0.0000, 0.7050],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0456],\n",
      "        [2.9148, 0.0000],\n",
      "        [0.0000, 0.7855]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.6771, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1818],\n",
      "        ...,\n",
      "        [0.0000, 0.4766],\n",
      "        [1.9870, 0.0000],\n",
      "        [0.0000, 0.3060]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1072, 0.1191],\n",
      "        [0.0000, 0.3092],\n",
      "        [0.0000, 1.0520],\n",
      "        ...,\n",
      "        [0.0000, 0.4906],\n",
      "        [1.9916, 0.0000],\n",
      "        [0.0000, 0.8664]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4008, 0.0000],\n",
      "        [0.0000, 0.9398],\n",
      "        [0.0000, 0.7681],\n",
      "        ...,\n",
      "        [0.0000, 1.0339],\n",
      "        [3.1098, 0.0000],\n",
      "        [0.0000, 1.1930]], grad_fn=<ReluBackward0>)\n",
      "epoch: 226  loss: 0.39970648\n",
      "tensor([[1.2889, 0.0000],\n",
      "        [0.6404, 0.3273],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2601],\n",
      "        [1.7124, 0.0000],\n",
      "        [0.0000, 0.6382]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2322, 0.0000],\n",
      "        [0.2801, 0.2747],\n",
      "        [0.0347, 0.0077],\n",
      "        ...,\n",
      "        [0.0000, 1.2115],\n",
      "        [1.4344, 0.0000],\n",
      "        [0.0000, 1.2449]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.6955, 0.0000],\n",
      "        [0.0000, 0.5086],\n",
      "        [2.6459, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5671],\n",
      "        [1.1290, 0.0000],\n",
      "        [0.0000, 0.5682]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9889, 0.0000],\n",
      "        [1.1449, 0.0000],\n",
      "        [0.0000, 1.0475],\n",
      "        ...,\n",
      "        [0.0000, 0.7426],\n",
      "        [1.2811, 0.0000],\n",
      "        [0.0000, 0.3186]], grad_fn=<ReluBackward0>)\n",
      "tensor([[3.1782, 0.0000],\n",
      "        [0.9050, 0.2913],\n",
      "        [0.0000, 0.7729],\n",
      "        ...,\n",
      "        [0.1421, 0.2468],\n",
      "        [1.7720, 0.0000],\n",
      "        [0.0000, 0.1117]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.5018, 0.0000],\n",
      "        [0.5225, 0.3784],\n",
      "        [0.0000, 0.5949],\n",
      "        ...,\n",
      "        [0.0000, 0.3138],\n",
      "        [3.0177, 0.0000],\n",
      "        [0.0000, 0.6313]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0595, 0.0000],\n",
      "        [0.0000, 0.3730],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5216],\n",
      "        [2.1931, 0.0000],\n",
      "        [0.0000, 1.1543]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4017, 0.0035],\n",
      "        [0.1853, 0.4041],\n",
      "        [0.0000, 0.0486],\n",
      "        ...,\n",
      "        [0.0000, 0.4344],\n",
      "        [1.6461, 0.0000],\n",
      "        [0.0000, 1.0217]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4975, 0.0000],\n",
      "        [0.0000, 0.1913],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.1257],\n",
      "        [2.5021, 0.0000],\n",
      "        [0.0000, 0.9278]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4431, 0.0000],\n",
      "        [0.0000, 0.4120],\n",
      "        [0.0000, 0.1049],\n",
      "        ...,\n",
      "        [0.0000, 0.5819],\n",
      "        [2.0499, 0.0000],\n",
      "        [0.0000, 1.0153]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7377, 0.0000],\n",
      "        [0.1820, 0.1553],\n",
      "        [0.4686, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6565],\n",
      "        [2.2030, 0.0000],\n",
      "        [0.0000, 0.5101]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.5013, 0.0000],\n",
      "        [0.9615, 0.0000],\n",
      "        [1.9715, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6378],\n",
      "        [1.8270, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1072, 0.0000],\n",
      "        [0.0000, 0.2327],\n",
      "        [0.0000, 1.0404],\n",
      "        ...,\n",
      "        [0.0000, 1.6187],\n",
      "        [0.6510, 0.0000],\n",
      "        [0.0000, 1.0123]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6261, 0.0000],\n",
      "        [0.2550, 0.0000],\n",
      "        [0.0000, 1.0492],\n",
      "        ...,\n",
      "        [0.0000, 0.6858],\n",
      "        [1.4414, 0.0000],\n",
      "        [0.0000, 1.0193]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7562, 0.0000],\n",
      "        [0.6895, 0.1572],\n",
      "        [0.0000, 0.1292],\n",
      "        ...,\n",
      "        [0.0000, 0.2345],\n",
      "        [2.5270, 0.0000],\n",
      "        [0.0000, 0.8505]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.7266, 0.0000],\n",
      "        [0.0000, 0.1062],\n",
      "        [0.0000, 0.2490],\n",
      "        ...,\n",
      "        [0.0000, 0.6269],\n",
      "        [0.9932, 0.0000],\n",
      "        [0.0000, 0.8613]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.8139, 0.0000],\n",
      "        [0.0000, 0.0511],\n",
      "        [0.0000, 0.1994],\n",
      "        ...,\n",
      "        [0.0000, 0.6028],\n",
      "        [1.6692, 0.0000],\n",
      "        [0.0000, 0.2477]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.5602, 0.0000],\n",
      "        [0.0000, 0.7856],\n",
      "        [1.7098, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.7285],\n",
      "        [2.9468, 0.0000],\n",
      "        [0.0000, 1.2299]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6527, 0.0000],\n",
      "        [1.1434, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.7214],\n",
      "        [3.6902, 0.0000],\n",
      "        [0.0000, 0.5428]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4533, 0.0000],\n",
      "        [0.0000, 0.1360],\n",
      "        [0.0000, 0.1560],\n",
      "        ...,\n",
      "        [0.0000, 1.1222],\n",
      "        [2.4350, 0.0000],\n",
      "        [0.0000, 0.7952]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5206, 0.0000],\n",
      "        [0.0000, 0.5388],\n",
      "        [0.0000, 0.2062],\n",
      "        ...,\n",
      "        [0.0000, 0.6673],\n",
      "        [1.4048, 0.0000],\n",
      "        [0.0000, 0.5570]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1843, 0.0000],\n",
      "        [0.0762, 0.1470],\n",
      "        [0.0000, 0.0422],\n",
      "        ...,\n",
      "        [0.0000, 0.6544],\n",
      "        [1.7225, 0.0000],\n",
      "        [0.0000, 1.1237]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8895, 0.0000],\n",
      "        [0.0000, 0.3993],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [3.5738, 0.0000],\n",
      "        [0.0000, 0.9137]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.9781, 0.0000],\n",
      "        [0.0000, 0.6940],\n",
      "        [0.0000, 0.0738],\n",
      "        ...,\n",
      "        [0.0000, 0.6591],\n",
      "        [2.4742, 0.0000],\n",
      "        [0.0000, 0.7871]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4374, 0.0000],\n",
      "        [0.4252, 0.3752],\n",
      "        [0.0000, 0.4377],\n",
      "        ...,\n",
      "        [0.0000, 0.9414],\n",
      "        [2.7915, 0.0000],\n",
      "        [0.0000, 0.3803]], grad_fn=<ReluBackward0>)\n",
      "epoch: 251  loss: 0.39046937\n",
      "tensor([[2.0839, 0.0000],\n",
      "        [0.9701, 0.0692],\n",
      "        [0.0000, 0.3582],\n",
      "        ...,\n",
      "        [0.0000, 1.1150],\n",
      "        [2.6054, 0.0000],\n",
      "        [0.0000, 0.9578]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7280, 0.0000],\n",
      "        [0.0000, 0.4027],\n",
      "        [0.0000, 0.2561],\n",
      "        ...,\n",
      "        [0.0000, 1.0910],\n",
      "        [0.8941, 0.0000],\n",
      "        [0.0000, 0.8989]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8269, 0.0000],\n",
      "        [0.3613, 0.1384],\n",
      "        [3.5028, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5943],\n",
      "        [1.9461, 0.0000],\n",
      "        [0.0000, 0.5281]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5319, 0.0000],\n",
      "        [0.0118, 0.2252],\n",
      "        [0.0000, 0.1506],\n",
      "        ...,\n",
      "        [0.0000, 0.2950],\n",
      "        [1.7292, 0.0000],\n",
      "        [0.0000, 0.4235]], grad_fn=<ReluBackward0>)\n",
      "tensor([[3.2631, 0.0000],\n",
      "        [0.0860, 0.3570],\n",
      "        [0.0000, 0.4465],\n",
      "        ...,\n",
      "        [0.0000, 0.6641],\n",
      "        [1.5566, 0.0000],\n",
      "        [0.0000, 0.7049]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2637, 0.0000],\n",
      "        [0.3121, 0.1234],\n",
      "        [0.0000, 0.5003],\n",
      "        ...,\n",
      "        [0.0000, 0.8763],\n",
      "        [1.2539, 0.0000],\n",
      "        [0.0000, 0.6751]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0555, 0.0000],\n",
      "        [0.0596, 0.0368],\n",
      "        [0.0000, 0.1178],\n",
      "        ...,\n",
      "        [0.0000, 0.4730],\n",
      "        [0.6482, 0.0000],\n",
      "        [0.0000, 0.4864]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.4286, 0.0000],\n",
      "        [0.0000, 0.9648],\n",
      "        [1.2324, 0.1606],\n",
      "        ...,\n",
      "        [0.0000, 0.8377],\n",
      "        [2.7500, 0.0000],\n",
      "        [0.0000, 0.7542]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8420, 0.0000],\n",
      "        [0.0000, 0.2749],\n",
      "        [0.0000, 0.6920],\n",
      "        ...,\n",
      "        [0.0000, 1.2207],\n",
      "        [1.7914, 0.0000],\n",
      "        [0.0000, 1.4486]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0409, 0.0000],\n",
      "        [0.0000, 1.0668],\n",
      "        [0.0000, 0.2714],\n",
      "        ...,\n",
      "        [0.0000, 0.6540],\n",
      "        [2.1115, 0.0000],\n",
      "        [0.0000, 1.0439]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.7690, 0.0000],\n",
      "        [0.0000, 0.7320],\n",
      "        [0.0000, 0.3756],\n",
      "        ...,\n",
      "        [0.0000, 1.1148],\n",
      "        [0.7136, 0.0000],\n",
      "        [0.0000, 0.7292]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5604, 0.0000],\n",
      "        [0.0000, 0.7997],\n",
      "        [0.0000, 1.1259],\n",
      "        ...,\n",
      "        [0.0000, 0.8008],\n",
      "        [1.6059, 0.0506],\n",
      "        [0.0000, 0.9966]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2448, 0.0000],\n",
      "        [0.0000, 0.6951],\n",
      "        [0.0000, 1.0448],\n",
      "        ...,\n",
      "        [0.0000, 0.9569],\n",
      "        [1.9992, 0.0000],\n",
      "        [0.0000, 0.6954]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0137, 0.0000],\n",
      "        [0.2237, 0.1311],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6511],\n",
      "        [1.4604, 0.0000],\n",
      "        [0.0000, 0.6104]], grad_fn=<ReluBackward0>)\n",
      "tensor([[4.1505, 0.0000],\n",
      "        [0.0000, 0.4955],\n",
      "        [0.0000, 1.0434],\n",
      "        ...,\n",
      "        [0.0000, 0.7826],\n",
      "        [0.7551, 0.0000],\n",
      "        [0.0000, 0.8135]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.8110, 0.0000],\n",
      "        [0.0000, 0.0416],\n",
      "        [0.0717, 0.1849],\n",
      "        ...,\n",
      "        [0.0000, 1.0164],\n",
      "        [2.2235, 0.0000],\n",
      "        [0.0000, 0.8276]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2241, 0.0000],\n",
      "        [0.0000, 0.2423],\n",
      "        [0.0000, 0.5884],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [2.6862, 0.0000],\n",
      "        [0.0000, 0.6411]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.6853, 0.0000],\n",
      "        [0.1615, 0.1501],\n",
      "        [0.7828, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2200],\n",
      "        [1.7409, 0.0000],\n",
      "        [0.0000, 0.2649]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5593, 0.0000],\n",
      "        [0.0000, 0.4538],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.7225],\n",
      "        [2.6973, 0.0000],\n",
      "        [0.0000, 0.2237]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1756, 0.0000],\n",
      "        [0.0000, 0.5668],\n",
      "        [0.2079, 0.2777],\n",
      "        ...,\n",
      "        [0.0000, 0.5263],\n",
      "        [1.1273, 0.0000],\n",
      "        [0.0000, 0.4389]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.8560, 0.0000],\n",
      "        [0.0000, 0.4355],\n",
      "        [0.0000, 0.6797],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [1.5938, 0.0000],\n",
      "        [0.0000, 0.1282]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6589, 0.0000],\n",
      "        [0.0000, 0.7707],\n",
      "        [1.3794, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5562],\n",
      "        [0.7521, 0.0000],\n",
      "        [0.0000, 0.6531]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8734, 0.0000],\n",
      "        [1.4970, 0.0000],\n",
      "        [0.0000, 0.6858],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [1.6750, 0.0000],\n",
      "        [0.0000, 0.3915]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0299, 0.0000],\n",
      "        [0.0000, 0.9181],\n",
      "        [0.0000, 0.6924],\n",
      "        ...,\n",
      "        [0.0000, 0.6517],\n",
      "        [3.4272, 0.0000],\n",
      "        [0.0000, 0.9948]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.1802, 0.0000],\n",
      "        [0.0000, 0.5833],\n",
      "        [0.0000, 0.4719],\n",
      "        ...,\n",
      "        [0.0000, 0.9052],\n",
      "        [1.0258, 0.0000],\n",
      "        [0.0000, 0.1880]], grad_fn=<ReluBackward0>)\n",
      "epoch: 276  loss: 0.38579708\n",
      "tensor([[2.1187, 0.0000],\n",
      "        [0.7851, 0.0882],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.7530],\n",
      "        [1.2655, 0.0000],\n",
      "        [0.0000, 1.0822]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9682, 0.0000],\n",
      "        [0.0000, 0.4841],\n",
      "        [0.0000, 0.1551],\n",
      "        ...,\n",
      "        [0.0000, 0.2338],\n",
      "        [0.4700, 0.0000],\n",
      "        [0.0000, 1.1689]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.6056, 0.0000],\n",
      "        [0.0000, 0.3786],\n",
      "        [0.0000, 1.6543],\n",
      "        ...,\n",
      "        [0.0000, 0.3671],\n",
      "        [1.2008, 0.0000],\n",
      "        [0.0000, 0.3186]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.3893, 0.0000],\n",
      "        [0.0000, 0.5335],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.8655],\n",
      "        [1.2630, 0.0000],\n",
      "        [0.0000, 0.5962]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.5285, 0.0000],\n",
      "        [0.0000, 0.4588],\n",
      "        [0.0000, 0.5829],\n",
      "        ...,\n",
      "        [0.0000, 1.1079],\n",
      "        [1.8379, 0.0000],\n",
      "        [0.0000, 0.4166]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2316, 0.0000],\n",
      "        [0.0000, 0.6392],\n",
      "        [0.0000, 0.6716],\n",
      "        ...,\n",
      "        [0.0000, 0.8506],\n",
      "        [2.1378, 0.0000],\n",
      "        [0.0000, 1.2974]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2691, 0.0000],\n",
      "        [0.3701, 0.3181],\n",
      "        [0.0000, 1.2140],\n",
      "        ...,\n",
      "        [0.0000, 0.7113],\n",
      "        [1.5929, 0.0522],\n",
      "        [0.0000, 0.6697]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2865, 0.0000],\n",
      "        [0.2342, 0.3276],\n",
      "        [0.0000, 1.0334],\n",
      "        ...,\n",
      "        [0.0000, 0.8063],\n",
      "        [2.4038, 0.0000],\n",
      "        [0.0000, 0.6615]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3637, 0.0000],\n",
      "        [1.1636, 0.0000],\n",
      "        [0.0000, 0.3542],\n",
      "        ...,\n",
      "        [0.0000, 0.7622],\n",
      "        [1.5177, 0.1618],\n",
      "        [0.0000, 0.3299]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7849, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.4934],\n",
      "        ...,\n",
      "        [0.0000, 0.8286],\n",
      "        [0.8682, 0.0000],\n",
      "        [0.0000, 0.3011]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4600, 0.0000],\n",
      "        [0.5495, 0.2434],\n",
      "        [0.0000, 0.4149],\n",
      "        ...,\n",
      "        [0.0000, 0.6296],\n",
      "        [1.3664, 0.0000],\n",
      "        [0.0000, 0.6533]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.9465, 0.0000],\n",
      "        [0.4933, 0.1606],\n",
      "        [0.0000, 0.5087],\n",
      "        ...,\n",
      "        [0.0000, 1.3955],\n",
      "        [2.4241, 0.0000],\n",
      "        [0.0000, 0.6585]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5101, 0.0000],\n",
      "        [0.1282, 0.3508],\n",
      "        [0.0000, 0.9526],\n",
      "        ...,\n",
      "        [0.0000, 0.2293],\n",
      "        [1.0180, 0.0000],\n",
      "        [0.0000, 1.1178]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.9220, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.5342],\n",
      "        ...,\n",
      "        [0.0000, 0.2707],\n",
      "        [2.7181, 0.0000],\n",
      "        [0.0000, 0.6419]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3701, 0.0000],\n",
      "        [0.0000, 0.2932],\n",
      "        [0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6073],\n",
      "        [2.0216, 0.0000],\n",
      "        [0.0000, 0.5948]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3919, 0.0000],\n",
      "        [0.0000, 0.4146],\n",
      "        [0.0000, 0.4244],\n",
      "        ...,\n",
      "        [0.0000, 0.3066],\n",
      "        [2.3563, 0.0000],\n",
      "        [0.0000, 0.7269]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3239, 0.0025],\n",
      "        [0.0000, 0.4283],\n",
      "        [0.0000, 0.1498],\n",
      "        ...,\n",
      "        [0.0000, 0.7895],\n",
      "        [0.5431, 0.2731],\n",
      "        [0.0000, 0.6380]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.4052, 0.0000],\n",
      "        [0.0000, 0.6485],\n",
      "        [0.0000, 1.1844],\n",
      "        ...,\n",
      "        [0.0000, 0.2940],\n",
      "        [0.8290, 0.0000],\n",
      "        [0.0000, 0.5925]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1985, 0.0000],\n",
      "        [0.0000, 0.5402],\n",
      "        [0.0000, 0.5961],\n",
      "        ...,\n",
      "        [0.0000, 0.5431],\n",
      "        [3.8955, 0.0000],\n",
      "        [0.0000, 0.8141]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2471, 0.0000],\n",
      "        [0.0000, 0.6913],\n",
      "        [0.0000, 0.1528],\n",
      "        ...,\n",
      "        [0.0000, 0.8184],\n",
      "        [1.1427, 0.0000],\n",
      "        [0.0000, 1.0927]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1957, 0.0000],\n",
      "        [0.0904, 0.1574],\n",
      "        [0.0000, 1.1797],\n",
      "        ...,\n",
      "        [0.0000, 0.4995],\n",
      "        [2.0766, 0.0000],\n",
      "        [0.0000, 1.1175]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.6413, 0.0000],\n",
      "        [0.0000, 0.3970],\n",
      "        [0.0000, 1.3282],\n",
      "        ...,\n",
      "        [0.0000, 1.2053],\n",
      "        [1.5270, 0.0000],\n",
      "        [0.0000, 0.3589]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.9053, 0.0000],\n",
      "        [0.0000, 0.1916],\n",
      "        [0.0000, 0.8025],\n",
      "        ...,\n",
      "        [0.0000, 1.1090],\n",
      "        [0.3695, 0.2351],\n",
      "        [0.0000, 0.5084]], grad_fn=<ReluBackward0>)\n",
      "tensor([[2.0674, 0.0000],\n",
      "        [0.0000, 0.4655],\n",
      "        [0.0000, 0.1663],\n",
      "        ...,\n",
      "        [0.0000, 1.0402],\n",
      "        [0.9459, 0.1540],\n",
      "        [0.0000, 1.2545]], grad_fn=<ReluBackward0>)\n",
      "epoch: 300  loss: 0.38355824\n",
      "\n",
      "Duration: 105 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 300\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    y_pred = model(cat_train, con_train)\n",
    "    print(y_pred)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # a neat trick to save screen space:\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3}  loss: {loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3}  loss: {loss.item():10.8f}') # print the last line\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Plot the Cross Entropy Loss against epochs\n",
    "Results may vary. The shape of the plot is what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CODE HERE\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.xlabel('epoch');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Evaluate the test set\n",
    "With torch set to <tt>no_grad</tt>, pass <tt>cat_test</tt> and <tt>con_test</tt> through the trained model. Create a validation set called \"y_val\". Compare the output to <tt>y_test</tt> using the loss function defined above. Results may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss: 0.37533039\n"
     ]
    }
   ],
   "source": [
    "# CODE HERE\n",
    "with torch.no_grad():\n",
    "    y_val = model(cat_test, con_test)\n",
    "    loss = criterion(y_val, y_test)\n",
    "\n",
    "# RUN THIS CODE TO COMPARE RESULTS\n",
    "print(f'CE Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss: 0.30774996\n"
     ]
    }
   ],
   "source": [
    "# TO EVALUATE THE TEST SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Calculate the overall percent accuracy\n",
    "Using a for loop, compare the argmax values of the <tt>y_val</tt> validation set to the <tt>y_test</tt> set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4255 out of 5000 = 85.10% correct\n"
     ]
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Feed new data through the trained model\n",
    "See if you can write a function that allows a user to input their own values, and generates a prediction.<br>\n",
    "<strong>HINT</strong>:<br>There's no need to build a DataFrame. You can use inputs to populate column variables, convert them to embeddings with a context dictionary, and pass the embedded values directly into the tensor constructors:<br>\n",
    "<pre>mar = input(\"What is the person's marital status? \")\n",
    "mar_d = dict(Divorced=0, Married=1, Married-spouse-absent=2, Never-married=3, Separated=4, Widowed=5)\n",
    "mar = mar_d[mar]\n",
    "cats = torch.tensor([..., ..., mar, ..., ...], dtype=torch.int64).reshape(1,-1)</pre>\n",
    "Make sure that names are put in alphabetical order before assigning numbers.\n",
    "\n",
    "Also, be sure to run <tt>model.eval()</tt> before passing new date through. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN YOUR CODE HERE:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the person's age? (18-90)  22\n",
      "What is the person's sex? (Male/Female) male\n",
      "What is the person's education level? (3-16) 12\n",
      "What is the person's marital status? married\n",
      "What is the person's workclass? private\n",
      "What is the person's occupation? sales\n",
      "How many hours/week are worked? (20-90)  40\n",
      "\n",
      "The predicted label is 0\n"
     ]
    }
   ],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
